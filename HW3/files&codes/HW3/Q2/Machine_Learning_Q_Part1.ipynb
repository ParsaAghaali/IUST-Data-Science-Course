{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Machine Learning Part 1\n",
    "\n",
    "\n",
    "__Use the provided building energy data to develop a model that can predict a building's Energy Star score, and then interpret the results to find the variables that are most predictive of the score.__\n",
    "\n",
    "This is a supervised, regression machine learning task: given a set of data with targets (in this case the score) included, we want to train a model that can learn to map the features (also known as the explanatory variables) to the target. \n",
    "\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "Although the exact implementation details can vary, the general structure of a machine learning project stays relatively constant: \n",
    "\n",
    "1. Data cleaning and formatting\n",
    "2. Exploratory data analysis\n",
    "3. Feature engineering and selection\n",
    "4. Establish a baseline and compare several machine learning models on a performance metric\n",
    "\n",
    "    __to be continued ...__\n",
    "\n",
    "Setting up the structure of the pipeline ahead of time lets us see how one step flows into the other. However, the machine learning pipeline is an iterative procedure and so we don't always follow these steps in a linear fashion.  We may revisit a previous step based on results from further down the pipeline. For example, while we may perform feature selection before building any models, we may use the modeling results to go back and select a different set of features. Or, the modeling may turn up unexpected results that mean we want to explore our data from another angle. Generally, you have to complete one step before moving on to the next, but don't feel like once you have finished one step the first time, you cannot go back and make improvements! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format\n",
    "\n",
    "__Code__\n",
    "\n",
    "fill where ever there is a missing spot like ... or your code and also be carful about null values!\n",
    "\n",
    "__Questions__\n",
    "\n",
    "Write your answer with enough explanation   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    " \n",
    "We will use the standard data science and machine learning libraries: `numpy`, `pandas`, and `scikit-learn`. We also use `matplotlib` and `seaborn` for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# No warnings about setting value on copy of slice\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "# Matplotlib visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 24\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Seaborn for visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Formatting\n",
    "\n",
    "## Load in the Data and Examine\n",
    "\n",
    "We will be loading our data into a pandas dataframe, one of the most useful data structures for data science. Think of it as a spreadsheet within Python that we can easily manipulate, clean, and visualize. [Pandas has many methods](http://pandas.pydata.org/pandas-docs/stable/) to help make the data science/machine learning pipeline as smooth as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data into a dataframe \n",
    "data = pd.read_csv('Energy_and_Water.csv')\n",
    "\n",
    "# Display top of dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the raw data, we can already see a number of issues we'll have to solve. First of all, there are 60 columns and we don't know what many of them mean! All we know from the problem statement is that we want to predict the number in the `score` column. Some of the other column definitions can be reasonably guessed, but others are difficult to understand .In machine learning, this isn't actually a problem, because we let the model decide which features are important. Sometimes we may not even be given column names or told what we are predicting. Nonetheless,  to understand the problem to the extent possible, and because we also want to interpret the model results, it would be a good idea to have some knowledge of the columns. \n",
    "\n",
    " we can get to [this pdf document](http://www.nyc.gov/html/gbee/downloads/misc/nyc_benchmarking_disclosure_data_definitions_2017.pdf) that details the meaning of every column. \n",
    "\n",
    "While we don't need to study every column, it would be a good idea to at least understand the target we want to predict. Here is the definition for the `score` target:\n",
    "\n",
    "    A 1-to-100 percentile ranking for specified building types, calculated in Portfolio Manager, based on self-reported energy usage for the reporting year.\n",
    "    \n",
    "That seems pretty straightforward: the Energy Star Score is a method of ranking buildings in terms of energy efficiency with 1 the worst and 100 the best. It is a relative percentile ranking which means buildings are scored relative to one another and should display a uniform distribution across the range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types and Missing Values\n",
    "\n",
    "The `dataframe.info` method is a quick way to assess the data by displaying the data types of each column and the number of non-missing values. Already from looking at the dataframe, there might be a problem because the missing values are coded as \"Not Available\" rather than as `np.nan` (not a number). This means the columns with numbers will not be represented as numeric because pandas converts columns with any strings values into columns of all strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See the column data types and non-missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, there are a number of columns with numbers that have been recorded as `object` datatypes. These will have to be converted to `float` datatype before we can do any numerical analysis. \n",
    "\n",
    "### Convert Data to Correct Types\n",
    "\n",
    "We convert the columns with numbers into numeric data types by replacing the \"Not Available\" entries with `np.nan` which can be interpreted as floats. Then we will convert the columns that contain numeric values (such as square feet or energy usage) into numeric datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences of 'Not Available' with numpy not a number\n",
    "data = None\n",
    "\n",
    "\n",
    "#Convert needed columns to numeric form\n",
    "#############\n",
    "# Your Code #\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for each column\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Now that we have the correct column datatypes, we can start analysis by looking at the percentage of missing values in each column. Missing values are fine when we do Exploratory Data Analysis, but they will have to be filled in for machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = None #df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = None #100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What seems to be the problem?\\\n",
    "asw:\\\n",
    "What can we do?\\\n",
    "asw:\\\n",
    "Do a bit of a research and decide on a threshold, and explain why is it suited.\\\n",
    "asw:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply your answer \n",
    "missing_df = ...\n",
    "missing_columns = ...\n",
    "print('We will remove %d columns.' % len(missing_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "data = data.drop(columns = list(missing_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the missing values will have to be imputed (filled-in) using an appropriate strategy before doing machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "[Exploratory Data Analysis (EDA) is an open-ended process where we make plots and calculate statistics in order to explore our data. The purpose is to to find anomalies, patterns, trends, or relationships. These may be interesting by themselves (for example finding a correlation between two variables) or they can be used to inform modeling decisions such as which features to use. In short, the goal of EDA is to determine what our data can tell us! EDA generally starts out with a high-level overview, and then narrows in to specific parts of the dataset once as we find interesting areas to examine. \n",
    "\n",
    "To begin the EDA, we will focus on a single variable, the Energy Star Score, because this is the target for our machine learning models. We can rename the column to `score` for simplicity and then start exploring this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Variable Plots\n",
    "\n",
    "A single variable (called [univariate](https://en.wikipedia.org/wiki/Univariate_(statistics)) plot shows the distribution of a single variable such as in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(8, 8)\n",
    "\n",
    "# Rename the score \n",
    "data = data.rename(columns = {'ENERGY STAR Score': 'score'})\n",
    "\n",
    "# Histogram of the Energy Star Score\n",
    "plt.style.use('fivethirtyeight')\n",
    "#Plot a histogram on 'score', and choose the right 'bins'! \n",
    "# your code\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of Buildings')\n",
    "plt.title('Energy Star Score Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first plot has already revealed some surprising (and suspicious) information! \n",
    "What interesting pint do we understand from this plot? (HINT: refer to score definition and pdf)\n",
    "asw:\n",
    "\n",
    "To contrast the Energy Star Score, we can look at the Energy Use Intensity (EUI), which is the total energy use divided by the square footage of the building. Here the energy usage is not self-reported, so this could be a more objective measure of the energy efficiency of a building. Moreover, this is not a percentile rank, so the absolute values are important and we would expect them to be approximately normally distributed with perhaps a few outliers on the low or high end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Plot of Site EUI\n",
    "figsize(8, 8)\n",
    "plt.hist(data['Site EUI (kBtu/ft²)'].dropna(), bins = 20, edgecolor = 'black')\n",
    "plt.xlabel('Site EUI')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Site EUI Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it seems we have another problem: outliers!\n",
    "The graph is incredibly skewed because of the presence of a few buildings with very high scores. It looks like we will have to take a slight detour to deal with the outliers. Let's look at the stats for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Site EUI (kBtu/ft²)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we understand the existence of outlier from this information?\\\n",
    "asw:\n",
    "\n",
    "__Find the one building (outlier) which is very different from the others__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Removing Outliers\n",
    "\n",
    "When we remove outliers, we want to be careful that we are not throwing away measurements just because they look strange. They may be the result of actual phenomenon that we should further investigate. When removing outliers, I try to be as conservative as possible, using the definition of an [extreme outlier](https://people.richland.edu/james/lecture/m170/ch03-pos.html): \n",
    "\n",
    "* On the low end, an extreme outlier is below  $\\text{First Quartile} -3 * \\text{Interquartile Range}$\n",
    "* On the high end, an extreme outlier is above $\\text{Third Quartile} + 3 * \\text{Interquartile Range}$\n",
    "\n",
    "In this case, I will only remove the single outlying point and see how the distribution looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first and third quartile\n",
    "first_quartile = ...\n",
    "third_quartile = ...\n",
    "\n",
    "# Interquartile range\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Remove outliers\n",
    "data = data[(data['Site EUI (kBtu/ft²)'] > (first_quartile - 3 * iqr)) &\n",
    "            (data['Site EUI (kBtu/ft²)'] < (third_quartile + 3 * iqr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Plot of Site EUI\n",
    "figsize(8, 8)\n",
    "plt.hist(data['Site EUI (kBtu/ft²)'].dropna(), bins = 20, edgecolor = 'black')\n",
    "plt.xlabel('Site EUI')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Site EUI Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the outliers, we can get back to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks a little less suspicious and is close to normally distributed with a long tail on the right side (it has a positive skew). \n",
    "\n",
    "Although this might be a more objective measure, our goal is still to predict the Energy Star Score, so we will move back to examining that variable. \n",
    "\n",
    "## Looking for Relationships\n",
    "\n",
    "In order to look at the effect of categorical variables on the score, we can make a [density plot](https://datavizcatalogue.com/methods/density_plot.html) colored by the value of the categorical variable. Density plots also show the distribution of a single variable and can be thought of as a smoothed histogram. \n",
    "If we color the density curves by a categorical variable, this will shows us how the distribution changes based on the class. \n",
    "\n",
    "The first plot we will make shows the distribution of scores by the property type. In order to not clutter the plot, we will limit the graph to building types that have more than 100 observations in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of buildings with more than 100 measurements\n",
    "types = data.dropna(subset=['score'])\n",
    "types = types['Largest Property Use Type'].value_counts()\n",
    "types = list(types[types.values > 100].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of distribution of scores for building categories\n",
    "figsize(12, 10)\n",
    "\n",
    "# Plot each building\n",
    "for b_type in types:\n",
    "    # Select the building type\n",
    "    subset = data[data['Largest Property Use Type'] == b_type]\n",
    "    \n",
    "    # Density plot of Energy Star scores\n",
    "    sns.kdeplot(subset['score'].dropna(),\n",
    "               label = b_type, shade = False, alpha = 0.8)\n",
    "    \n",
    "# label the plot\n",
    "plt.xlabel('Energy Star Score', size = 20)\n",
    "plt.ylabel('Density', size = 20)\n",
    "plt.title('Density Plot of Energy Star Scores by Building Type', size = 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the `Largest Property Use Type` or the building type affects our wanted `score` (the negative scores on the graph are an artifact of the [kernel density estimation](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) procedure). \\\n",
    "\n",
    "\n",
    "How can we convert this categorical variable to a useful form of data for our Model?\\\n",
    "asw:\n",
    "\n",
    "To examine another categorical variable, borough, we can make the same graph, but this time colored by the borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of boroughs with more than 100 observations\n",
    "boroughs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of distribution of scores for boroughs\n",
    "figsize(12, 10)\n",
    "\n",
    "# Plot each borough distribution of scores\n",
    "for borough in boroughs:\n",
    "    # your code #\n",
    "    ...\n",
    "# label the plot\n",
    "plt.xlabel('Energy Star Score', size = 20); plt.ylabel('Density', size = 20); \n",
    "plt.title('Density Plot of Energy Star Scores by Borough', size = 28);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is boroughs different from building type?\\\n",
    "asw:\\\n",
    "Is it useful to use this column as well?\\\n",
    "asw:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations between Features and Target\n",
    "\n",
    "In order to quantify correlations between the features (variables) and the target, use the Pearson similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all correlations and sort \n",
    "correlations_data = ...\n",
    "\n",
    "# Print the most negative correlations\n",
    "print(correlations_data.head(15), '\\n')\n",
    "\n",
    "# Print the most positive correlations\n",
    "print(correlations_data.tail(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strong negative correlations between the features and the target. The most negative correlations with the score are the different categories of Energy Use Intensity (EUI), `Site EUI (kBtu/ft²)` and  `Weather Normalized Site EUI (kBtu/ft²)` (these vary slightly in how they are calculated). The EUI is the amount of energy used by a building divided by the square footage of the buildings and is meant to be a measure of the efficiency of a building with a lower score being better. Intuitively, these correlations then make sense: as the EUI increases, the Energy Star Score tends to decrease. \n",
    "\n",
    "To account for possible non-linear relationships, You can take square root and natural log transformations of the features and then calculate the correlation coefficients with the score. \n",
    "\n",
    "In the following code, we take log and square root transformations of the numerical variables, the correlations between all of the features and the score, and display the top 15 most positive and top 15 most negative correlations.\n",
    "\n",
    "Remember to transform useful Categorical Columns to numerical form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numeric columns\n",
    "numeric_subset = data.select_dtypes('number')\n",
    "\n",
    "# Create columns with square root and log of numeric columns\n",
    "for col in numeric_subset.columns:\n",
    "    # Skip the Energy Star Score column\n",
    "    if col == 'score':\n",
    "        next\n",
    "    else:\n",
    "        ...\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_subset = data[...] # informative categorical columns \n",
    "\n",
    "# Transform to numerical\n",
    "categorical_subset = ...\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Drop buildings without an energy star score\n",
    "features = features.dropna(subset = ['score'])\n",
    "\n",
    "# Find correlations with the score \n",
    "correlations = features.corr()['score'].dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most negative correlations\n",
    "correlations.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most positive correlations\n",
    "correlations.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the features, the strongest relationships are still those related to Energy Use Intensity (EUI). The log and square root transformations do not seem the have resulted in any stronger relationships. There are no strong positive linear relationships although we do see that a building type of office (`Largest Property Use Type_Office`) is slightly positively correlated with the score. This variable is a one-hot encoded representation of the categorical variables for building type.\n",
    "\n",
    "We can use these correlations in order to perform feature selection (coming up in a little bit). Right now, let's graph the most significant correlation (in terms of absolute value) in the dataset which is `Site EUI (kBtu/ft^2)`. We can color the graph by the building type to show how that affects the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Variable Plots\n",
    "\n",
    "In order to visualize the relationship between two variables, we use a scatterplot. We can also include additional variables using aspects such as color of the markers or size of the markers. Here we will plot two numeric variables against one another and use color to represent a third categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12, 10)\n",
    "\n",
    "# Extract the building types\n",
    "features['Largest Property Use Type'] = data.dropna(subset = ['score'])['Largest Property Use Type']\n",
    "\n",
    "# Limit to building types with more than 100 observations (from previous code)\n",
    "features = features[features['Largest Property Use Type'].isin(types)]\n",
    "\n",
    "# Use seaborn to plot a scatterplot of Score vs Log Source EUI\n",
    "sns.lmplot('Site EUI (kBtu/ft²)', 'score', \n",
    "          hue = 'Largest Property Use Type', data = features,\n",
    "          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n",
    "          size = 12, aspect = 1.2)\n",
    "\n",
    "# Plot labeling\n",
    "plt.xlabel(\"Site EUI\", size = 28)\n",
    "plt.ylabel('Energy Star Score', size = 28)\n",
    "plt.title('Energy Star Score vs Site EUI', size = 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear negative relationship between the Site EUI and the score. The relationship is not perfectly linear (it looks with a correlation coefficient of -0.7, but it does look like this feature will be important for predicting the score of a building. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot\n",
    "\n",
    "As a final exercise for exploratory data analysis, we can make a pairs plot between several different variables. The Pairs Plot is a great way to examine many variables at once as it shows scatterplots between pairs of variables and histograms of single variables on the diagonal. \n",
    "\n",
    "Using the seaborn `PairGrid` function, we can map different plots on to the three aspects of the grid. The upper triangle will have scatterplots, the diagonal will show histograms, and the lower triangle will show both the correlation coefficient between two variables and a 2-D kernel density estimate of the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns to  plot\n",
    "plot_data = features[['score', 'Site EUI (kBtu/ft²)', \n",
    "                      'Weather Normalized Source EUI (kBtu/ft²)', \n",
    "                      'log_Total GHG Emissions (Metric Tons CO2e)']]\n",
    "\n",
    "# Replace the inf with nan\n",
    "plot_data = plot_data.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Rename columns \n",
    "plot_data = plot_data.rename(columns = {'Site EUI (kBtu/ft²)': 'Site EUI', \n",
    "                                        'Weather Normalized Source EUI (kBtu/ft²)': 'Weather Norm EUI',\n",
    "                                        'log_Total GHG Emissions (Metric Tons CO2e)': 'log GHG Emissions'})\n",
    "\n",
    "# Drop na values\n",
    "plot_data = plot_data.dropna()\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 3)\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, color = 'red', alpha = 0.6)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(plt.hist, color = 'red', edgecolor = 'black')\n",
    "\n",
    "# Bottom is correlation and density plot\n",
    "grid.map_lower(corr_func)\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.Reds)\n",
    "\n",
    "# Title for entire plot\n",
    "plt.suptitle('Pairs Plot of Energy Data', size = 36, y = 1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the relationships in the plot, we can look for where the variables in one row intersect with the variables in one column. For example, to find the relationship between score and the log of GHG Emissions, we look at the score column and find the log GHG Emissions row. At the intersection (the lower left plot) we see that the score has a -0.35 correlation coefficient with this varible. If we look at the upper right plot, we can see a scatterplot of this relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection\n",
    "\n",
    "Now that we have explored the trends and relationships within the data, we can work on engineering a set of features for our models. We can use the results of the EDA to inform this feature engineering. In particular, we learned the following from EDA which can help us in engineering/selecting features:\n",
    "\n",
    "* The score distribution varies by building type and to a lesser extent by borough. Although we will focus on numerical features, we should also include these two categorical features in the model. \n",
    "\n",
    "\n",
    "Did taking the log transformation of features result in significant increases in the linear correlations between features and the score?\n",
    "asw:\n",
    "\n",
    "\n",
    "Before we get any further, we should define what feature engineering and selection are! These definitions are informal and have considerable overlap, but I like to think of them as two separate processes:\n",
    "\n",
    "* __[Feature Engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)__: The process of taking raw data and extracting or creating new features that allow a machine learning model to learn a mapping beween these features and the target. This might mean taking transformations of variables, such as we did with the log and square root, or one-hot encoding categorical variables so they can be used in a model. Generally, I think of feature engineering as __adding__ additional features derived from the raw data.\n",
    "* __[Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)__: The process of choosing the most relevant features in your data. \"Most relevant\" can depend on many factors, but it might be something as simple as the highest correlation with the target, or the features with the [most variance](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html). In feature selection, we remove features that do not help our model learn the relationship between features and the target. This can help the model generalize better to new data and results in a more interpretable model. Generally, I think of feature selection as __subtracting__ features so we are left with only those that are most important.\n",
    "\n",
    "[Feature engineering and selection](https://www.featurelabs.com/blog/secret-to-data-science-success/) often has the highest returns on time invested in a machine learning problem. It can take quite a while to get right, but is often more important than the exact algorithm and hyperparameters used for the model. If we don't feed the model the correct data, then we are setting it up to fail and we should not expect it to learn! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will take the following steps for feature engineering:\n",
    "\n",
    "* Select only the numerical variables and categorical variables \n",
    "* Add in the log transformation of the numerical variables\n",
    "\n",
    "For feature selection, we will do the following:\n",
    "\n",
    "* Remove [collinear features](https://statinfer.com/204-1-9-issue-of-multicollinearity-in-python/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code selects the numeric features, adds in log transformations of all the numeric features, selects and one-hot encodes the categorical features, and joins the sets of features together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data\n",
    "features = data.copy()\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_subset = ...\n",
    "\n",
    "# Create columns with log of numeric columns\n",
    "for col in numeric_subset.columns:\n",
    "    # Skip the Energy Star Score column\n",
    "    if col == 'score':\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['log_' + col] = ...\n",
    "        \n",
    "# Select the categorical columns\n",
    "categorical_subset = data[[...]]\n",
    "\n",
    "# Transformation\n",
    "categorical_subset = ...\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have 11319 observations (buildings) with many different features (one column is the score). Not all of these features are likely to be important for predicting the score, and several of these features are also redundant because they are highly correlated. We will deal with this second issue below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Collinear Features\n",
    "\n",
    "Highly [collinear features](http://psychologicalstatistics.blogspot.com/2013/11/multicollinearity-and-collinearity-in.html) have a significant correlation coefficent between them. \n",
    "\n",
    "Could plot and you give an example of two collinear related features? \\\n",
    "asw:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1 = ...\n",
    "col_2 = ...\n",
    "plot_data = data[[col_1, col_2]].dropna()\n",
    "\n",
    "plt.plot(plot_data[col_1], plot_data[col_2], 'bo')\n",
    "plt.xlabel('col1'); plt.ylabel('col2')\n",
    "plt.title('col_1, col_2 R = %0.4f' % np.corrcoef(data[[col_1,col_2]].dropna(), rowvar=False)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While variables in a dataset are usually correlated to a small degree, highly collinear variables can be redundant in the sense that we only need to retain one of the features to give our model the necessary information.\n",
    "\n",
    "Removing collinear features is a method to reduce model complexity by decreasing the number of features and can help to increase model generalization.  It can also help us to interpret the model because we only have to worry about a single variable, such as EUI, rather than how both EUI and weather normalized EUI affect the score. \n",
    "\n",
    "There are a number of methods for removing collinear features, such as using the [Variance Inflation Factor](http://www.statisticshowto.com/variance-inflation-factor/). We will use a simpler metric, and remove features that have a correlation coefficient above a certain threshold with each other (not with the score because we want variables that are highly correlated with the score!) For a more thorough discussion of removing collinear variables, check out [this notebook on Kaggle](https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity/code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code removes the collinear features based on a threshold we select for the correlation coefficients by removing one of the two features that are compared. It also prints the correlations that it removes so we can see the effect of adjusting the threshold. We will use a threshold of 0.6 which removes one of a pair of features if the correlation coefficient between the features exceeds this value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between Energy Star Score\n",
    "    y = x['score']\n",
    "    x = x.drop(columns = ['score'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "           # Your code here#\n",
    "           ...\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu/ft²)', \n",
    "                          'Water Use (All Water Sources) (kgal)',\n",
    "                          'log_Water Use (All Water Sources) (kgal)',\n",
    "                          'Largest Property Use Type - Gross Floor Area (ft²)'])\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['score'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove the collinear features above a specified correlation coefficient\n",
    "features = remove_collinear_features(features, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any columns with all na values\n",
    "features  = features.dropna(axis=1, how = 'all')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final dataset now has 64 features (one of the columns is the target). This is still quite a few, but mostly it is because we have one-hot encoded the categorical variables. Moreover, while a large number of features may be problematic for models such as linear regression, models such as the random forest perform implicit feature selection and automatically determine which features are important during traning. There are other feature selection steps to take, but for now we will keep all the features we have and see how the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Feature Selection\n",
    "\n",
    "There are plenty of more methods for [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html). Some popular methods include [principal components analysis (PCA)](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) which transforms the features into a reduced number of dimensions that preserve the greatest variance, or [independent components analysis (ICA)](http://cs229.stanford.edu/notes/cs229-notes11.pdf) which aims to find the independent sources in a set of features. However, while these methods are effective at reducing the number of features, they create new features that have no physical meaning and hence make interpreting a model nearly impossible. \n",
    "\n",
    "These methods are very helpful for dealing with high-dimensional data and I would suggest [reading more on the topic](https://machinelearningmastery.com/feature-selection-machine-learning-python/) if you plan on dealing with machine learning problems! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into Training and Testing Sets\n",
    "\n",
    "In machine learning, we always need to separate our features into two sets:\n",
    "\n",
    "1. __Training set__ which we provide to our model during training along with the answers so it can learn a mapping between the features and the target. \n",
    "2. __Testing set__ which we use to evaluate the mapping learned by the model. The model has never seen the answers on the testing set, but instead, must make predictions using only the features. As we know the true answers for the test set, we can then compare the test predictions to the true test targets to ghet an estimate of how well our model will perform when deployed in the real world. \n",
    "\n",
    "For our problem, we will first extract all the buildings without an Energy Star Score (we don't know the true answer for these buildings so they will not be helpful for training or testing). Then, we will split the buildings with an Energy Star Score into a testing set of 30% of the buildings, and a training set of 70% of the buildings. \n",
    "\n",
    "Splitting the data into a random training and testing set is simple using scikit-learn. We can set the random state of the split to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the buildings with no score and the buildings with a score\n",
    "no_score = features[features['score'].isna()]\n",
    "score = features[features['score'].notnull()]\n",
    "\n",
    "print(no_score.shape)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the features and targets\n",
    "features = score.drop(columns='score')\n",
    "targets = pd.DataFrame(score['score'])\n",
    "\n",
    "# Replace the inf and -inf with nan (required for later imputation)\n",
    "features = features.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Split into 70% training and 30% testing set\n",
    "X, X_test, y, y_test = train_test_split(features, targets, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1858 buildings with no score, 6622 buildings with a score in the training set, and 2839 buildings with a score in the testing set. We have one final step to take in this notebook: determining a naive baseline for our models to beat! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to establish a naive baseline before we beginning making machine learning models. If the models we build cannot outperform a naive guess then we might have to admit that machine learning is not suited for this problem. This could be because we are not using the right models, because we need more data, or because there is a simpler solution that does not require machine learning. Establishing a baseline is crucial so we do not end up building a machine learning model only to realize we can't actually solve the problem.\n",
    "\n",
    "For a regression task, a good naive baseline is to predict the median value of the target on the training set for all examples on the test set. This is simple to implement and sets a relatively low bar for our models: if they cannot do better than guessing the medin value, then we will need to rethink our approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric: Mean Absolute Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    # your code\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make the median guess and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_guess = np.median(y)\n",
    "\n",
    "print('The baseline guess is a score of %0.2f' % baseline_guess)\n",
    "print(\"Baseline Performance on the test set: MAE = %0.4f\" % mae(y_test, baseline_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows our average estimate on the test set is off by about 25 points. The scores are between 1 and 100 so this means the average error from a naive method if about 25%. The naive method of guessing the median training value provides us a low baseline for our models to beat! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we carried out the first three [steps of a machine learning](https://towardsdatascience.com/the-7-steps-of-machine-learning-2877d7e5548e?gi=30cd995093a9) problem:\n",
    "\n",
    "1. Cleaned and formatted the raw data \n",
    "2. Performed an exploratory data analysis\n",
    "3. Developed a set of features to train our model using feature engineering and feature selection\n",
    "\n",
    "We also completed the crucial task of establishing a baseline metric so we can determine if our model is better than guessing! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the no scores, training, and testing data\n",
    "no_score.to_csv('no_score.csv', index = False)\n",
    "X.to_csv('training_features.csv', index = False)\n",
    "X_test.to_csv('testing_features.csv', index = False)\n",
    "y.to_csv('training_labels.csv', index = False)\n",
    "y_test.to_csv('testing_labels.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
